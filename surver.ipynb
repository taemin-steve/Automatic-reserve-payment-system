{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pred Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.322369247674942\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import glob\n",
    "import cv2 as cv\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from tqdm.auto import tqdm\n",
    "import timm\n",
    "\n",
    "\n",
    "review_img = cv.imread('1.png')\n",
    "product_img = cv.imread('2.png')\n",
    "\n",
    "def pred_distance(review_img_path, product_img_path):\n",
    "    df = pd.DataFrame(columns=['review_img_path','product_img_path', 'label'])\n",
    "    df['review_img_path'] = [review_img_path]\n",
    "    df['product_img_path'] = [product_img_path]\n",
    "    df['label'] = [0]\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    CFG = {\n",
    "        'IMG_SIZE':224,\n",
    "        'EPOCHS':1,\n",
    "        'LEARNING_RATE':3e-4,\n",
    "        # 'LEARNING_RATE':10,\n",
    "        'BATCH_SIZE':1,\n",
    "        'SEED':41\n",
    "    }\n",
    "    \n",
    "    class SiameseNetworkDataset(Dataset):\n",
    "        def __init__(self,review_img_path,product_img_path,label,transform=None):\n",
    "            self.review_img_path = review_img_path\n",
    "            self.product_img_path = product_img_path\n",
    "            self.label = label\n",
    "            self.transform = transform\n",
    "\n",
    "        def __getitem__(self,index):\n",
    "            # review_img = cv.imread(self.review_img_path[index])\n",
    "            # product_img = cv.imread(self.product_img_path[index])\n",
    "            review_img = self.review_img_path[index]\n",
    "            product_img = self.product_img_path[index]\n",
    "            review_img = cv.resize(review_img, (CFG['IMG_SIZE'], CFG['IMG_SIZE']))\n",
    "            product_img = cv.resize(product_img, (CFG['IMG_SIZE'], CFG['IMG_SIZE']))\n",
    "\n",
    "            if self.transform is not None:\n",
    "                review_img  = self.transform(image=review_img)['image']\n",
    "                product_img  = self.transform(image=product_img)['image']\n",
    "\n",
    "            return review_img, product_img, self.label[index]\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.review_img_path)\n",
    "        \n",
    "    train_transform = A.Compose([\n",
    "                            A.Resize(CFG['IMG_SIZE'],CFG['IMG_SIZE']),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "    \n",
    "    val_dataset = SiameseNetworkDataset(df[\"review_img_path\"].values, df[\"product_img_path\"].values, df[\"label\"].values, train_transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "    class BaseModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(BaseModel, self).__init__()\n",
    "            self.backbone = timm.create_model('efficientnet_b0', pretrained=False)\n",
    "            self.classifier = nn.Linear(1000, 50)\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.ReLU = nn.ReLU(inplace=False)\n",
    "\n",
    "        def forward(self, x, y):\n",
    "            x = self.backbone(x)\n",
    "            x = self.classifier(x)\n",
    "\n",
    "            y = self.backbone(y)\n",
    "            y = self.classifier(y)\n",
    "\n",
    "            z = F.pairwise_distance(x, y, keepdim = True)\n",
    "            return z\n",
    "        \n",
    "    def validation(model, val_loader, device):\n",
    "        model.eval()\n",
    "        pred_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for review_img, product_img, labels in iter(val_loader):\n",
    "                review_img = review_img.float().to(device)\n",
    "                product_img = product_img.float().to(device)\n",
    "    \n",
    "                \n",
    "                pred = model(review_img, product_img)\n",
    "                pred = pred.detach().cpu().numpy().tolist()\n",
    "                pred_list += pred\n",
    "            \n",
    "        return pred_list \n",
    "        \n",
    "    def train(model,  val_loader,  device):\n",
    "        model = model.to(device)\n",
    "\n",
    "        model.train()\n",
    "        prediction = validation(model, val_loader, device)\n",
    "\n",
    "        return prediction\n",
    "        \n",
    "    \n",
    "    model = BaseModel()\n",
    "    model.load_state_dict(torch.load('./distance_EffNetBase_E_Contra.pt'))\n",
    "    model.eval()\n",
    "\n",
    "    prediction = train(model,  val_loader, device)\n",
    "    \n",
    "    return prediction[0][0]\n",
    "    \n",
    "print(pred_distance(review_img, product_img))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def Pose_Estimation(img_path):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    CFG = {\n",
    "        'EPOCHS':1,\n",
    "        'LEARNING_RATE':3e-8,\n",
    "        # 'LEARNING_RATE':10,\n",
    "        'BATCH_SIZE':1,\n",
    "        'SEED':41\n",
    "        }\n",
    "    base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
    "\n",
    "    options = vision.PoseLandmarkerOptions(\n",
    "        base_options=base_options,\n",
    "        num_poses = 22,\n",
    "        output_segmentation_masks=False)\n",
    "\n",
    "    detector = vision.PoseLandmarker.create_from_options(options)\n",
    "    \n",
    "    save_x = []\n",
    "    save_y = []\n",
    "    save_z = []\n",
    "    save_presence = []\n",
    "    \n",
    "        \n",
    "    img = mp.Image.create_from_file(img_path)\n",
    "    pose_landmarks_list = detector.detect(img).pose_landmarks\n",
    "        \n",
    "    if not pose_landmarks_list:\n",
    "        return False\n",
    "            \n",
    "    save_x.append([i.x for i in pose_landmarks_list[0][11:33]])\n",
    "    save_y.append([i.y for i in pose_landmarks_list[0][11:33]])\n",
    "    save_z.append([i.z for i in pose_landmarks_list[0][11:33]])\n",
    "    save_presence.append([i.presence for i in pose_landmarks_list[0][11:33]])\n",
    "    \n",
    "    df = pd.DataFrame(columns=['img_path','label'])\n",
    "    \n",
    "    df['img_path'] = None\n",
    "    df['label'] = 0\n",
    "    df['landmark_x'] = save_x\n",
    "    df['landmark_y'] = save_y\n",
    "    df['landmark_z'] = save_z\n",
    "    df['landmark_presence'] = save_presence\n",
    "    \n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, img_path ,landmark_x, landmark_y,\tlandmark_z,\tlandmark_presence, label):\n",
    "            self.img_path = img_path\n",
    "            self.landmark_x = landmark_x\n",
    "            self.landmark_y = landmark_y\n",
    "            self.landmark_z= landmark_z\n",
    "            self.landmark_presence = landmark_presence\n",
    "            self.label = label\n",
    "\n",
    "        def __getitem__(self,index):\n",
    "\n",
    "            result = np.concatenate((self.landmark_x[index] , self.landmark_y[index] , self.landmark_z[index] , self.landmark_presence[index]), axis=0)\n",
    "            return result, self.label[index]\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.landmark_x )\n",
    "    \n",
    "    val_dataset = CustomDataset(df[\"img_path\"].values, df[\"landmark_x\"].values, df[\"landmark_y\"].values, df[\"landmark_z\"].values,df[\"landmark_presence\"].values, df[\"label\"].values)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "    \n",
    "    class BaseModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(BaseModel, self).__init__()\n",
    "            self.classifier1 = nn.Linear(88, 20)\n",
    "            # self.classifier1 = nn.Linear(22, 2)\n",
    "            self.ReLU = nn.ReLU(inplace=True)\n",
    "            self.classifier2 = nn.Linear(20, 2)\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.classifier1(x)\n",
    "            x = self.ReLU(x)\n",
    "            x = self.classifier2(x)\n",
    "\n",
    "            return F.log_softmax(x, dim=1)\n",
    "        \n",
    "    def validation(model, criterion, val_loader, device):\n",
    "        model.eval()\n",
    "        preds= []\n",
    "        with torch.no_grad():\n",
    "            for landmark_list, labels in iter(val_loader):\n",
    "                landmark_list = landmark_list.float().to(device)\n",
    "                pred = model(landmark_list)\n",
    "                preds += pred.detach().argmax(1).cpu().numpy().tolist()\n",
    "        return preds\n",
    "\n",
    "\n",
    "    def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "        model = model.to(device)\n",
    "        criterion = nn.NLLLoss(weight=torch.tensor([0.01, 0.99]), reduction=\"sum\").to(device)\n",
    "        best_model = None\n",
    "        for epoch in range(0, CFG['EPOCHS']):\n",
    "            model.train()\n",
    "            label = validation(model, criterion, val_loader, device)\n",
    "        return best_model , label\n",
    "    \n",
    "    model = BaseModel()\n",
    "    model.load_state_dict(torch.load('./Pose_Estimate_new.pt'))\n",
    "    model.eval()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, threshold_mode='abs', min_lr=1e-8, verbose=True)\n",
    "    infer_model, label = train(model, optimizer, None, val_loader, scheduler, device)\n",
    "    \n",
    "    if label[0] == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "print(Pose_Estimation('./1.png'))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# surver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './19.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 43\u001b[0m\n\u001b[1;32m     37\u001b[0m image_input1 \u001b[39m=\u001b[39m gr\u001b[39m.\u001b[39mImage(\u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfilepath\u001b[39m\u001b[39m\"\u001b[39m,label\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m리뷰 이미지 올리세요\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m image_input2 \u001b[39m=\u001b[39m gr\u001b[39m.\u001b[39mImage(\u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfilepath\u001b[39m\u001b[39m\"\u001b[39m,label\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m구매 제품을 선택하세요\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m demo \u001b[39m=\u001b[39m gr\u001b[39m.\u001b[39;49mInterface(\n\u001b[1;32m     44\u001b[0m                     fn\u001b[39m=\u001b[39;49mimage_classifier,\n\u001b[1;32m     45\u001b[0m                     inputs\u001b[39m=\u001b[39;49m [image_input1,image_input2],\n\u001b[1;32m     46\u001b[0m                     outputs\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     47\u001b[0m                     examples \u001b[39m=\u001b[39;49m [product_1,product_2,product_3,product_4]\n\u001b[1;32m     48\u001b[0m                     )\n\u001b[1;32m     49\u001b[0m demo\u001b[39m.\u001b[39mlaunch(share\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/SAM/lib/python3.9/site-packages/gradio/interface.py:475\u001b[0m, in \u001b[0;36mInterface.__init__\u001b[0;34m(self, fn, inputs, outputs, examples, cache_examples, examples_per_page, live, interpretation, num_shap, title, description, article, thumbnail, theme, css, allow_flagging, flagging_options, flagging_dir, flagging_callback, analytics_enabled, batch, max_batch_size, _api_mode, **kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattach_interpretation_events(\n\u001b[1;32m    468\u001b[0m         interpretation_btn,\n\u001b[1;32m    469\u001b[0m         interpretation_set,\n\u001b[1;32m    470\u001b[0m         input_component_column,\n\u001b[1;32m    471\u001b[0m         interpret_component_column,\n\u001b[1;32m    472\u001b[0m     )\n\u001b[1;32m    474\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattach_flagging_events(flag_btns, clear_btn)\n\u001b[0;32m--> 475\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_examples()\n\u001b[1;32m    476\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_article()\n\u001b[1;32m    478\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_config_file()\n",
      "File \u001b[0;32m/opt/conda/envs/SAM/lib/python3.9/site-packages/gradio/interface.py:791\u001b[0m, in \u001b[0;36mInterface.render_examples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    785\u001b[0m non_state_inputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    786\u001b[0m     c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_components \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(c, State)\n\u001b[1;32m    787\u001b[0m ]\n\u001b[1;32m    788\u001b[0m non_state_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    789\u001b[0m     c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_components \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(c, State)\n\u001b[1;32m    790\u001b[0m ]\n\u001b[0;32m--> 791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexamples_handler \u001b[39m=\u001b[39m Examples(\n\u001b[1;32m    792\u001b[0m     examples\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexamples,\n\u001b[1;32m    793\u001b[0m     inputs\u001b[39m=\u001b[39;49mnon_state_inputs,  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    794\u001b[0m     outputs\u001b[39m=\u001b[39;49mnon_state_outputs,  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    795\u001b[0m     fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn,\n\u001b[1;32m    796\u001b[0m     cache_examples\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache_examples,\n\u001b[1;32m    797\u001b[0m     examples_per_page\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexamples_per_page,\n\u001b[1;32m    798\u001b[0m     _api_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_mode,\n\u001b[1;32m    799\u001b[0m     batch\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch,\n\u001b[1;32m    800\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/SAM/lib/python3.9/site-packages/gradio/helpers.py:54\u001b[0m, in \u001b[0;36mcreate_examples\u001b[0;34m(examples, inputs, outputs, fn, cache_examples, examples_per_page, _api_mode, label, elem_id, run_on_click, preprocess, postprocess, batch)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_examples\u001b[39m(\n\u001b[1;32m     39\u001b[0m     examples: List[Any] \u001b[39m|\u001b[39m List[List[Any]] \u001b[39m|\u001b[39m \u001b[39mstr\u001b[39m,\n\u001b[1;32m     40\u001b[0m     inputs: IOComponent \u001b[39m|\u001b[39m List[IOComponent],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     batch: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m ):\n\u001b[1;32m     53\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Top-level synchronous function that creates Examples. Provided for backwards compatibility, i.e. so that gr.Examples(...) can be used to create the Examples component.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     examples_obj \u001b[39m=\u001b[39m Examples(\n\u001b[1;32m     55\u001b[0m         examples\u001b[39m=\u001b[39;49mexamples,\n\u001b[1;32m     56\u001b[0m         inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m     57\u001b[0m         outputs\u001b[39m=\u001b[39;49moutputs,\n\u001b[1;32m     58\u001b[0m         fn\u001b[39m=\u001b[39;49mfn,\n\u001b[1;32m     59\u001b[0m         cache_examples\u001b[39m=\u001b[39;49mcache_examples,\n\u001b[1;32m     60\u001b[0m         examples_per_page\u001b[39m=\u001b[39;49mexamples_per_page,\n\u001b[1;32m     61\u001b[0m         _api_mode\u001b[39m=\u001b[39;49m_api_mode,\n\u001b[1;32m     62\u001b[0m         label\u001b[39m=\u001b[39;49mlabel,\n\u001b[1;32m     63\u001b[0m         elem_id\u001b[39m=\u001b[39;49melem_id,\n\u001b[1;32m     64\u001b[0m         run_on_click\u001b[39m=\u001b[39;49mrun_on_click,\n\u001b[1;32m     65\u001b[0m         preprocess\u001b[39m=\u001b[39;49mpreprocess,\n\u001b[1;32m     66\u001b[0m         postprocess\u001b[39m=\u001b[39;49mpostprocess,\n\u001b[1;32m     67\u001b[0m         batch\u001b[39m=\u001b[39;49mbatch,\n\u001b[1;32m     68\u001b[0m         _initiated_directly\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     70\u001b[0m     utils\u001b[39m.\u001b[39msynchronize_async(examples_obj\u001b[39m.\u001b[39mcreate)\n\u001b[1;32m     71\u001b[0m     \u001b[39mreturn\u001b[39;00m examples_obj\n",
      "File \u001b[0;32m/opt/conda/envs/SAM/lib/python3.9/site-packages/gradio/helpers.py:201\u001b[0m, in \u001b[0;36mExamples.__init__\u001b[0;34m(self, examples, inputs, outputs, fn, cache_examples, examples_per_page, _api_mode, label, elem_id, run_on_click, preprocess, postprocess, batch, _initiated_directly)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch \u001b[39m=\u001b[39m batch\n\u001b[1;32m    200\u001b[0m \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39mset_directory(working_directory):\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_examples \u001b[39m=\u001b[39m [\n\u001b[1;32m    202\u001b[0m         [\n\u001b[1;32m    203\u001b[0m             component\u001b[39m.\u001b[39mpostprocess(sample)\n\u001b[1;32m    204\u001b[0m             \u001b[39mfor\u001b[39;00m component, sample \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(inputs, example)\n\u001b[1;32m    205\u001b[0m         ]\n\u001b[1;32m    206\u001b[0m         \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m examples\n\u001b[1;32m    207\u001b[0m     ]\n\u001b[1;32m    208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnon_none_processed_examples \u001b[39m=\u001b[39m [\n\u001b[1;32m    209\u001b[0m     [ex \u001b[39mfor\u001b[39;00m (ex, keep) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(example, input_has_examples) \u001b[39mif\u001b[39;00m keep]\n\u001b[1;32m    210\u001b[0m     \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_examples\n\u001b[1;32m    211\u001b[0m ]\n\u001b[1;32m    212\u001b[0m \u001b[39mif\u001b[39;00m cache_examples:\n",
      "File \u001b[0;32m/opt/conda/envs/SAM/lib/python3.9/site-packages/gradio/helpers.py:202\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch \u001b[39m=\u001b[39m batch\n\u001b[1;32m    200\u001b[0m \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39mset_directory(working_directory):\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_examples \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 202\u001b[0m         [\n\u001b[1;32m    203\u001b[0m             component\u001b[39m.\u001b[39mpostprocess(sample)\n\u001b[1;32m    204\u001b[0m             \u001b[39mfor\u001b[39;00m component, sample \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(inputs, example)\n\u001b[1;32m    205\u001b[0m         ]\n\u001b[1;32m    206\u001b[0m         \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m examples\n\u001b[1;32m    207\u001b[0m     ]\n\u001b[1;32m    208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnon_none_processed_examples \u001b[39m=\u001b[39m [\n\u001b[1;32m    209\u001b[0m     [ex \u001b[39mfor\u001b[39;00m (ex, keep) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(example, input_has_examples) \u001b[39mif\u001b[39;00m keep]\n\u001b[1;32m    210\u001b[0m     \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_examples\n\u001b[1;32m    211\u001b[0m ]\n\u001b[1;32m    212\u001b[0m \u001b[39mif\u001b[39;00m cache_examples:\n",
      "File \u001b[0;32m/opt/conda/envs/SAM/lib/python3.9/site-packages/gradio/helpers.py:203\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch \u001b[39m=\u001b[39m batch\n\u001b[1;32m    200\u001b[0m \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39mset_directory(working_directory):\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_examples \u001b[39m=\u001b[39m [\n\u001b[1;32m    202\u001b[0m         [\n\u001b[0;32m--> 203\u001b[0m             component\u001b[39m.\u001b[39;49mpostprocess(sample)\n\u001b[1;32m    204\u001b[0m             \u001b[39mfor\u001b[39;00m component, sample \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(inputs, example)\n\u001b[1;32m    205\u001b[0m         ]\n\u001b[1;32m    206\u001b[0m         \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m examples\n\u001b[1;32m    207\u001b[0m     ]\n\u001b[1;32m    208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnon_none_processed_examples \u001b[39m=\u001b[39m [\n\u001b[1;32m    209\u001b[0m     [ex \u001b[39mfor\u001b[39;00m (ex, keep) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(example, input_has_examples) \u001b[39mif\u001b[39;00m keep]\n\u001b[1;32m    210\u001b[0m     \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_examples\n\u001b[1;32m    211\u001b[0m ]\n\u001b[1;32m    212\u001b[0m \u001b[39mif\u001b[39;00m cache_examples:\n",
      "File \u001b[0;32m/opt/conda/envs/SAM/lib/python3.9/site-packages/gradio/components.py:1634\u001b[0m, in \u001b[0;36mImage.postprocess\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m   1632\u001b[0m     \u001b[39mreturn\u001b[39;00m processing_utils\u001b[39m.\u001b[39mencode_pil_to_base64(y)\n\u001b[1;32m   1633\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(y, (\u001b[39mstr\u001b[39m, Path)):\n\u001b[0;32m-> 1634\u001b[0m     \u001b[39mreturn\u001b[39;00m processing_utils\u001b[39m.\u001b[39;49mencode_url_or_file_to_base64(y)\n\u001b[1;32m   1635\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1636\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot process this value as an Image\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/SAM/lib/python3.9/site-packages/gradio/processing_utils.py:70\u001b[0m, in \u001b[0;36mencode_url_or_file_to_base64\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m encode_url_to_base64(path)\n\u001b[1;32m     69\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m encode_file_to_base64(path)\n",
      "File \u001b[0;32m/opt/conda/envs/SAM/lib/python3.9/site-packages/gradio/processing_utils.py:94\u001b[0m, in \u001b[0;36mencode_file_to_base64\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_file_to_base64\u001b[39m(f):\n\u001b[0;32m---> 94\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(f, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     95\u001b[0m         encoded_string \u001b[39m=\u001b[39m base64\u001b[39m.\u001b[39mb64encode(file\u001b[39m.\u001b[39mread())\n\u001b[1;32m     96\u001b[0m         base64_str \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(encoded_string, \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './19.png'"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import mask_image\n",
    "\n",
    "\n",
    "def image_classifier(review_img, product_img):\n",
    "    width = 30\n",
    "    height = 50\n",
    "    \n",
    "    if Pose_Estimation(review_img):\n",
    "        review_img = cv.imread(review_img)\n",
    "        product_img = cv.imread(product_img)\n",
    "        \n",
    "        img_list  = mask_image.mask_image([review_img, product_img])\n",
    "        preprocessed_review = img_list[0]\n",
    "        preprocessed_product = img_list[1] \n",
    "        \n",
    "        if (preprocessed_review is None) or (preprocessed_product is None):\n",
    "            return str(\"이미지를 분류하는데 실패하였습니다\")\n",
    "        \n",
    "        distance = pred_distance(preprocessed_review, preprocessed_product)\n",
    "        \n",
    "        if distance > 0.75:\n",
    "            return str(\"구매하신 제품이 아닙니다.\")\n",
    "        else:\n",
    "            return str(\"적립금이 지급되었습니다.\")\n",
    "    else:\n",
    "        return str(\"전신사진이여야 합니다.\")\n",
    "    # return str(Pose_Estimation(review_img))\n",
    "\n",
    "\n",
    "product_1 = [None, \"./19.jpg\"]\n",
    "product_2 = [None, \"./20.jpg\"]\n",
    "product_3 = [None, \"./21.jpg\"]\n",
    "product_4 = [None, \"./18.jpg\"]\n",
    "\n",
    "\n",
    "image_input1 = gr.Image(type =\"filepath\",label= \"리뷰 이미지 올리세요\")\n",
    "image_input2 = gr.Image(type =\"filepath\",label= \"구매 제품을 선택하세요\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "demo = gr.Interface(\n",
    "                    fn=image_classifier,\n",
    "                    inputs= [image_input1,image_input2],\n",
    "                    outputs=\"text\",\n",
    "                    examples = [product_1,product_2,product_3,product_4]\n",
    "                    )\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
